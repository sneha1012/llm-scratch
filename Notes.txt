LLM is a neural network designed to understand, generate and respond like humans do. deep neural networks models trained on several large text data.
large in large language models refers to both the model parameters size and and also the large datset it is trained on. Parameter \s are wieghts adjusted during training and optimised to predict the next wor in the sequence.

LLMs utilise architecture called Transformers inherently. Which also allows to pay specific attention to the ifferetn parts of the input making it better for human like generation and understanding.
In deep learning, we do not need to choose the best suited features manualy like in machine learning but we do still need labels like - spam or nospam.
LLms are best for mostly automating tasks, be it nay type of unstructured data involving parsing and generating text. LLM from scratch for better fine-tuning or pretuning our other large language models for our domain specifc tasks or datasets.
1. (Trainigng Stage)pretraining suing unlabeled dataset for tasks like text completion d few shot capabilites 2. pretrained LLM can be fine tuned using a smaller labeled dataset.

Categories for Fine-tuning of LLMs:
1. Instructional fine-tuning - Instructions  + query groups like (prompt for translating - answer after translation)
2. assification fine-tuning - associated labels


##Transfomer architecture (Submodules)
1. Encoder - Encodes the input texts into vectore(numerical representation) embeddings which capture the real sense 
2. Decoder - Takes the encoded embeddings to generate the actual output required 
Encoders and Decoders consists of many self-attention mechanism layers. which allows the LLM to weigh the importance of different words r token sequence in relative positions ot each other.

Some LLMs are still based on COnvulational and recurrent architecture. 
Tokens = number of units of text wwhere the number of tokens often represent the numbers of words and punctuations in the text. Tokenization is the process of converting the text into tokens (words and punctuations)
The dataset usually refers to including syntax, semantics and context and general knowledge. The next word prediction task is like a self-supervised learning, form of self-labelling. "Autoagressive" models use their previous outputs for next inputs.

##What does building an LLM from scratch looks like; It has three stages:
Stage 1: Building block
1. Data Preparation and sampling - tokenization, token id creation 
2. Attenstion mechanism
3. LLM Architecture (Encoder / Decoder/ both) 

Stage 2: Foundational Model (unlabelled data)
1. Training Loop
2. Model Evaluation
3. load pretrained weights 

Stage 3: FineTuned LLM (Classifier or Personal Assistant which is instructional)
1. Either Instructional Dataset 
2. Or Dataset with Class Labels

A few Steps to get the steps started for prepping the text/ data:
1. Splitting into words/tokens
2. byte pair encoding as a more advanced Tokenization process.
We can then move on to sampling training example with sliding window approach, and finally converting tokens into vectors that can be fed into Large L Models/

Since the text is categorical, we need to basically change/convert them into continous times vectors, the process is called embeddings. We often use Word2Vec,
where we cluster the similar types os vector embeddings considering they all have similar meaning whenever projected in 2 dimensional space, this is usually done for machine learning models though. LLms usually prodcue their own embeddings and 
update them while training each layer. we can use python's "re.split" to split text on whitespaces. 

eg:
import re
example text = "hello my name is Sneha Maurya"
result = re.split(r'(\s)', text)  #only on whitspaces  ## Captilisation of text helps LLM's understand the concept of proper and common nouns. 
or result = re.split(r'([,.]|\s)'), text #on periods and whitespaces now. 

Now if we want to completly remove whitespaces as tokens too, we can use 'strip'
result = [item in item in result if item.strip()], Though we intentionaly keep the whitespaces for generation like code in python which is indentation sensitive.





